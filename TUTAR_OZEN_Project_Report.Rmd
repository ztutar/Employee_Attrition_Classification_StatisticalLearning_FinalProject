---
title: "Statistical Learning Final Project"
subtitle: "Employee Attrition Classification"

author: [Zeynep TUTAR - 2106038, Aysenur Oya ÖZEN - 0000000]

date: "09-07-2024"
toc: true
toc-own-page: true
number_sections: true
df_print: kable
fontsize: 10pt
lang: en-GB
titlepage: true
titlepage-background: "img/unipdbg.pdf"
header-center: "\\leftmark"
header-right: "\\thetitle"
footer-left: "\\hspace{1cm}"
footer-center: "Page \\thepage"
footer-right: "\\hspace{1cm}"
hyperrefoptions:
  - linktoc=all
biblio-style: apsr
colorlinks: true

editor_options: 
  markdown: 
    wrap: 72
    

header-includes:
   - \usepackage{setspace}
   - \newpage
   - \usepackage{hyperref}
   - \usepackage{multirow}
   - \usepackage{amsmath}
   - \usepackage{algorithm}
   - \usepackage{algorithmic}
   - \usepackage{authblk}
   - \usepackage{graphicx}
   - \usepackage{fancyvrb}
   - \RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{breaklines=true,breakanywhere=true}



output:
  pdf_document:
    template: "template/eisvogel.tex"
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      tidy.opts = list(width.cutoff = 72),
                      tidy = TRUE, comment = '',
                      fig.path='figs/',
                      cache = TRUE, cache.path = '_cache/')
# fig.keep = 'all',

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "small","\n\n", x, "\n\n \\normalsize")
})
```

# Introduction to Dataset

The aim of this project is to develop two predictive models to determine
employee attrition of a company. The dataset[^1] used for this project
is a simulated dataset designed for the analysis and prediction of
employee attrition. It contains detailed information about various
aspects of an employee's profile, including demographics, job-related
features, and personal circumstances.The dataset contains 74,498
samples. Each record includes a unique Employee ID and features that
influence employee attrition. The goal is to understand the factors
contributing to attrition and develop predictive models to identify
at-risk employees.

[^1]: <https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset/data>

The dataset is already split into train and test but in order to better
understand the data, it is crucial to analyse the dataset as a whole.

```{r import_data}

# import the train and test datasets
data_train <- read.csv("data/train.csv", stringsAsFactors=TRUE)
data_test <- read.csv("data/test.csv", stringsAsFactors = TRUE)

# merge the datasets
data <- rbind(data_train, data_test)
attach(data)
```

## Description of the Features

The features of the dataset are presented below:

-   **Employee ID:** A unique identifier assigned to each employee.

-   **Age:** The age of the employee, ranging from 18 to 60 years.

-   **Gender:** The gender of the employee

-   **Years at Company:** The number of years the employee has been
    working at the company.

-   **Monthly Income:** The monthly salary of the employee, in dollars.

-   **Job Role:** The department or role the employee works in, encoded
    into categories such as Finance, Healthcare, Technology, Education,
    and Media.

-   **Work-Life Balance:** The employee's perceived balance between work
    and personal life, (Poor, Below Average, Good, Excellent)

-   **Job Satisfaction:** The employee's satisfaction with their job:
    (Very Low, Low, Medium, High)

-   **Performance Rating:** The employee's performance rating: (Low,
    Below Average, Average, High)

-   **Number of Promotions:** The total number of promotions the
    employee has received.

-   **Distance from Home:** The distance between the employee's home and
    workplace, in miles.

-   **Education Level:** The highest education level attained by the
    employee: (High School, Associate Degree, Bachelor’s Degree,
    Master’s Degree, PhD)

-   **Marital Status:** The marital status of the employee: (Divorced,
    Married, Single)

-   **Job Level:** The job level of the employee: (Entry, Mid, Senior)

-   **Company Size:** The size of the company the employee works for:
    (Small,Medium,Large)

-   **Company Tenure:** The total number of years the employee has been
    working in the industry.

-   **Remote Work:** Whether the employee works remotely: (Yes or No)

-   **Leadership Opportunities:** Whether the employee has leadership
    opportunities: (Yes or No)

-   **Innovation Opportunities:** Whether the employee has opportunities
    for innovation: (Yes or No)

-   **Company Reputation:** The employee's perception of the company's
    reputation: (Very Poor, Poor,Good, Excellent)

-   **Employee Recognition:** The level of recognition the employee
    receives:(Very Low, Low, Medium, High)

-   **Attrition:** Whether the employee has left the company, encoded as
    0 (stayed) and 1 (Left).

# Data Analysis

In order to develop predictive models, first it is necessary to perform
exploratory data analysis (EDA) and modify the format of the data if
necessary.

```{r libraries}
# installing required libraries
library(class)
library(e1071)
library(car)
library(corrplot)
library(glmnet)
library(dplyr)
library(pROC)
library(knitr)
```

```{r summary}

# Descriptive statistics of DataFrame
summary(data)

# Data types of columns
str(data)

```

## Data Preprocessing

To prepare the dataset for further analysis, several data preprocessing
steps are performed:

1.  Removing features

```{r ID_drop}
# first column contains Employee IDs, so not necessary for analysis
data <- data[, !names(data) %in% "Employee.ID"]

# EXPLANATION!!!!!!!!!!
data <- data[, !names(data) %in% "Company.Tenure"]

```

2.  Numeric and categorical value separation

```{r num_cat}

numeric_vars <- sapply(data, is.numeric)

categoric_vars <- sapply(data, function(x) is.factor(x) || is.character(x))

```

3.  Handling missing values

```{r NA_val}
# Missing Values --- No null Values
na_summary <- sapply(data, function(x) sum(is.na(x)))
na_summary

```

### Categorical Features

```{r cat_f}

#Categorical feature names
categoric_var_names <- names(data)[categoric_vars]

# Categorical value distributions
for (var in categoric_var_names) {
  cat("\nDistribution of", var, ":\n")
  print(table(data[[var]]))
}

# Categorical value distribution -- barplot
par(mfrow = c(2, 3))  
for (cat_var in categoric_var_names) {
  barplot(table(data[[cat_var]]), main=paste(cat_var, "Distribution"), xlab=cat_var, col="firebrick4")
}

```

```{r, include=FALSE}
par(mfrow = c(1, 1)) 
```

### Numeric Features

```{r num_f}
# Numeric value summary
summary(data[, numeric_vars])

# Numeric feature names
numeric_var_names <- names(data)[numeric_vars]

# Numeric features--hist graph
plots_per_page <- 6
num_plots <- length(numeric_var_names)
num_pages <- ceiling(num_plots / plots_per_page)

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(3, 2))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    hist(data[[num_var]], main=paste(num_var, "Distribution"), xlab=num_var, col="firebrick4", breaks=30)
    
    plot_index <- plot_index + 1
  }
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

### Target Values

```{r target}

# Target value distribution
par(mfrow = c(1, 2)) 
barplot(table(data$Attrition), main="Attrition Count", xlab="Attrition", ylab="Count", col=c("firebrick4", "rosybrown2"))

# Target value distribution -- Pie chart
attrition_table <- table(data$Attrition)
attrition_df <- as.data.frame(attrition_table)
colnames(attrition_df) <- c("Attrition", "Count")
attrition_df$Percentage <- round(100 * attrition_df$Count / sum(attrition_df$Count), 1)

pie(attrition_df$Count, labels=paste(attrition_df$Attrition, " - ", attrition_df$Percentage, "%"), 
    col=c("firebrick4","rosybrown2"),
    main="Attrition Distribution", cex=1.2, radius=0.8)
 
```

```{r, include=FALSE}
par(mfrow = c(1, 1)) 
```


```{r}
# Density plots
cat_var <- "Attrition"

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 3))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    plot(density(data[[num_var]][data[[cat_var]] == "Left"], na.rm = TRUE), col="red", main=paste(num_var, "Density \nby", cat_var), xlab=num_var, ylab="Density")
    lines(density(data[[num_var]][data[[cat_var]] == "Stayed"], na.rm = TRUE), col="blue")
    legend("topright", legend=c("Left", "Stayed"), col=c("red", "blue"), lty=1, cex=0.8)
    plot_index <- plot_index + 1
  }
}

```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

### Outliers

```{r outliers}
# Outlier Analysis 
par(mfrow = c(2, 3)) 
for (num_var in numeric_var_names) {
  boxplot(data[[num_var]], main=paste(num_var, "Boxplot"), xlab=num_var, col="firebrick4")
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```


```{r target-num}

# Target Visualization with Numeric features Outlier Check -- boxplot
cat_var <- "Attrition"

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 3))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    boxplot(data[[num_var]] ~ data[[cat_var]], main=paste(num_var, "\nby", cat_var), xlab=cat_var, ylab=num_var, col="firebrick4")
    plot_index <- plot_index + 1
  }
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```



```{r}
# Function to identify outliers using IQR
identify_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  outliers <- x[x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)]
  return(outliers)
}

# Identify and show outliers for each numeric variable
outliers_list <- list()
for (var in numeric_var_names) {
  outliers <- identify_outliers(data[[var]])
  outliers_list[[var]] <- outliers
  cat("\nOutliers in", var, ":\n")
  print(outliers)
}
```


```{r}
# Plot histograms and highlight outliers

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 3))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    
    hist(data[[num_var]], main=paste(num_var, "Distribution"), xlab=num_var, col="firebrick4", breaks=30)
    outliers <- outliers_list[[num_var]]
    if (length(outliers) > 0) {
      points(outliers, rep(0, length(outliers)), col="rosybrown2", pch=16)
    }
    
    plot_index <- plot_index + 1
  }
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

As a result of the analysis, the following observations were made
regarding the characteristics of the data:

-   

-   

-   

-   

-   

-   

-   

## Features vs. Target

### Categorical Features vs. Target

### Numerical Features vs. Target

## Correlation Matrix

```{r Cor.Cov.}
# Cor. and Cov.
cov_matrix <- cov(data[, numeric_var_names])
cor_matrix <- cor(data[, numeric_var_names])

print("Covariance Matrix:")
print(cov_matrix)
print("Correlation Matrix:")
print(cor_matrix)

corrplot(cor_matrix, method = 'number', type = "upper", tl.col = "black", tl.srt = 45)
```

### Partial Correlation Matrices

# Data Preparation

After completing the data analysis steps, it is necessary to prepare the data for model development.

## Handling Categorical Features

In order to use the categorical features in the model, we need to
convert categorical features to numeric (ordinal or nominal)
representations.

```{r mapping}

# Ordinal mappings:
balance.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Work.Life.Balance <- balance.map[as.numeric(data$Work.Life.Balance)]

satisfaction.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Job.Satisfaction <- satisfaction.map[as.numeric(data$Job.Satisfaction)]

performance.map <- c('Low'= 1, 'Below Average'= 2, 'Average'= 3, 'High'= 4)
data$Performance.Rating <- performance.map[as.numeric(data$Performance.Rating)]

level.map <- c('Entry'= 1, 'Mid'= 2, 'Senior'= 3)
data$Job.Level <- level.map[as.numeric(data$Job.Level)]

reputation.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Company.Reputation <- reputation.map[as.numeric(data$Company.Reputation)]

recognition.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Employee.Recognition <- recognition.map[as.numeric(data$Employee.Recognition)]

size.map <- c('Small'= 1, 'Medium'= 2, 'Large'= 3)
data$Company.Size <- size.map[as.numeric(data$Company.Size)]

# Nominal mappings:
# Create dummy variables for nominal data
data_numeric <- model.matrix( ~ ., data = data) 

# Convert the resulting matrix back to a data frame
data_numeric <- as.data.frame(data_numeric)[,-1]  # -1 to remove the intercept column

```

## Train-Test-Split

Before splitting the data into training and test, first features and
target should be defined.

```{r x_y_split}
# Splitting data into features and target:
X <- data_numeric[, !(colnames(data_numeric) %in% c("Employee.ID", "AttritionStayed"))]

y <- data_numeric$AttritionStayed
```

Now, we can split the dataset for modelling.

```{r train_test_split}
set.seed(42)

trainIndex <- sample(1:nrow(X), 0.8*nrow(X))

# 80% of data is used for training
X.train <- X[trainIndex,]
y.train <- y[trainIndex]

# 20% of data is used for testing
X.test <- X[-trainIndex,]
y.test <- y[-trainIndex]
```

Before moving to modelling step, it is beneficial to check the
dimensions and balance of the datasets.

```{r sanity_check}
# Number of samples in train data
dim(X.train)
train.size <- dim(X.train)[1]

# Number of samples in test data
dim(X.test)
test.size <- dim(X.test)[1]

# Proportion of stayed employees for train data
prop.table(table(y.train))

# Proportion of stayed employees for test data
prop.table(table(y.test))
```

We can observe that the train and test datasets are balanced within
themselves. Also the train data is representative of test data.

# Predictive Classification Models

Predictive classification models are a type of machine learning
algorithm used to predict the category or class label of new, unseen
instances based on historical data. These models are trained using a
labelled dataset where the input features (independent variables) are
associated with known class labels (dependent variable). The goal of the
model is to learn the relationship between the features and the class
labels so that it can accurately classify new data points into one of
the predefined categories.

In this project we aim to find the risk of an employee leaving the
company (class 0) and the factors affecting employee retention. So we
will develop several classification models and examine their
performances.

## Logistic Regression

The logistic regression model estimates the odds of the dependent
variable occurring and applies the logit (log-odds) transformation to
express this relationship.

$$
g(\pi_i) = \text{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right)     \in (-\infty, +\infty)
$$

### Basic Logistic Classifier

```{r}
# First of all we check the model statistics with all the features
glm.FULL <- glm(y.train ~ ., data = X.train, family = binomial)

summary(glm.FULL)
```


The above model statistics indicate that p-value of Employee Recognition is above 0.5 indicating that this feature is insignificant to the results. Additionally, some Job.Roles and Monthly. Income also have high p-values indicating that their effect on
Attrition is less significant compared to other features. However, for
now we would like to keep all the features in the model and apply
feature selection later.

In order to understand how well the model fits the data we can make use
of $R^2$ statistics. $R^2$ provides an indication of how well the
independent variables in the model explain the variability of the
dependent variable. A higher $R^2$ value indicates a better fit of the
model to the data. The formula for $R^2$ is:

$$ R^2 = 1 - \frac{RSS}{ESS} $$

Where:

-   ${RSS}$ is the sum of squares of the residuals (the differences
    between observed and predicted values), i.e. the deviance of the
    fitted model
-   ${ESS}$ is the total sum of squares due to regression (the
    differences between the observed values and the mean of the observed
    values)

```{r full_r2}
R2 <- 1 - (summary(glm.FULL)$deviance/summary(glm.FULL)$null.deviance)
R2
```

With the full model the value of $R^2$ 0.2352228 indicates that
approximately 23.52% of the variance in the target can be explained by
the features in the model. Since 23.52% is relatively low, it suggests
that the model is not capturing much of the underlying pattern in the
data.

Multicollinearity can be a reason for a low $R^2$ value, as it can make
it difficult to determine the individual effect of each predictor on the
target. Calculating the Variance Inflation Factor (VIF) can help to
check for multicollinearity among the features.

```{r vif}
vif.FULL <- data.frame(features = names(vif(glm.FULL)), VIF = vif(glm.FULL),row.names = NULL)
vif.FULL[order(-vif.FULL$VIF), ]
```

A VIF value of 1 indicates no correlation, values between 1 and 5
indicate moderate correlation and values above 5 suggest significant
multicollinearity, which can lead to unreliable coefficient estimates.
Most variables have VIF values close to 1, indicating very low or no
multicollinearity. A few variables have VIF values between 1 and 5,
suggesting moderate multicollinearity, which may not pose serious issues
but should be monitored. These variables include Job.RoleTechnology,
Job.RoleHealthcare, Monthly.Income, Job.RoleFinance,
Marital.StatusSingle, Marital.StatusMarried, Job.RoleMedia and
Years.at.Company. These are mostly dummy features of nominal variables
and dummy variables are often correlated because they represent
categories of the same nominal variable.

### Logistic Regression with Backward Stepwise Search

Backward variable selection is a greedy search algorithm used to develop
a predictive model by iteratively removing the least significant
features The goal is to find the best subset of features that contribute
to the model while eliminating those that do not improve its
performance.

We can use the step() function to perform backward stepwise search. As a
regularization criteria we can either use BIC or AIC. But BIC statistic
generally places a heavier penalty on models with many variables, and
hence results in the selection of smaller models than AIC. In this case,
the model has moderate amount of variables so we can use AIC statistic.
Since already one dummy variable for each category is dropped, removing
additional dummy variables can lead to loss of information so we need to
be cautious.

```{r step_back}
# Backward Stepwise Search with AIC statistics
glm.BACKWARD <- step(glm.FULL, direction="backward", k=2, trace=0, steps=20)
summary(glm.BACKWARD)

vif.BAKWARD <- data.frame(features = names(vif(glm.BACKWARD)), VIF = vif(glm.BACKWARD),row.names = NULL)

vif.BAKWARD[order(-vif.BAKWARD$VIF), ]

```

It looks like backward stepwise search dropped "Job.RoleFinance",
"Job.RoleHealthcare", "Job.RoleTechnology" and "Employee.Recognition" from the model. These features were also the ones with highest p-values so it seems logical. But oddly the model kept the "Job.RoleMedia" feature so this tells us that having a job role in Media may be more significant to employee Attrition than other job roles for this dataset.

Although we decreased the VIF scores, p-values and AIC score,
unfortunately, RSS increased and $R^2$ dropped to 0.2352002. So, the
models ability to capture the underlying pattern within the dataset
decreased.

### Logistic Regression with Shrinkage Method

Shrinkage methods are techniques used in regression analysis to prevent
overfitting by introducing a penalty for large coefficients. These
methods "shrink" the coefficients towards zero, effectively reducing
their variance and, in turn, enhancing the model's generalizability. Two
most used shrinkage methods are Ridge Regression and Lasso Regression.

-   Ridge Regression adds a penalty to the regression model that shrinks
    all the coefficients, and is useful when we want to keep all
    features in the model.

-   Lasso Regression introduces a penalty that can shrink some
    coefficients to zero. This method is useful for feature selection,
    yielding sparse models by retaining only the most important
    features.

-   Elastic Net is a hybrid regularization technique that includes both the Ridge
    and Lasso penalties, allowing for variable selection (like Lasso) and
    handling multicollinearity (like Ridge). It aims to incorporate the
    strengths of both methods, providing a more flexible approach.

Choosing the value of $lambda$ (the tuning parameter) is crucial because
it determines the strength of the penalty applied to the coefficients.
We can choose the best value of $lambda$ using k-fold cross-validation method.


```{r NET}
# Elastic Net Regression with alpha=0.05 (more weight on Ridge Reg.)
set.seed(42)

glm.ELNET <- cv.glmnet(as.matrix(X.train), y.train, alpha = 0.05, 
                       family = "binomial", type.measure = "class")

plot(glm.ELNET)

best.lamda <- glm.ELNET$lambda.min
```

### Comparison of Logistic Classifiers

For Logistic Regression we defined 3 Logistic classifiers. In order to identify the best model we can compare their performance on the test sets to see how well they captured the underlying pattern of the data and their ability to generalize to new data.

```{r log_compare}
# 1. Basic Logistic Classifier
glm.FULL.predict <- predict(glm.FULL, newdata = X.test, type = "response")

# 2. Feature Selection with Backward Stepwise Search
glm.BACKWARD.predict <- predict(glm.BACKWARD, newdata = X.test, type = "response")

# 3. Elastic Net Shrinkage Method
glm.ELNET.predict <- predict(glm.ELNET, newx = as.matrix(X.test), 
                             type = "response", s = best.lamda)
```

We can use ROC curve and AUC values to compare the models.The ROC curve is a tool for assessing the performance of binary classification models, plotting true positive rate against false positive rate at various thresholds. The Area Under the Curve (AUC) provides a measure of the model's ability to predict the target values, with higher values indicating better performance.

```{r roc_auc, echo=FALSE, fig.keep='last'}

par(pty="s")
invisible(roc(y.test, glm.FULL.predict, plot=TRUE, legacy.axes=TRUE, percent=TRUE, 
    xlab="False Positive %", ylab="True Positive %", col="firebrick4", lwd=4,
    print.auc=TRUE, print.auc.y=60, print.auc.x=30, quiet = TRUE))

plot.roc(y.test, glm.BACKWARD.predict, add=TRUE, legacy.axes=TRUE,
                   percent=TRUE, xlab="False Positive %", ylab="True Positive %",
                   col="indianred3", lwd=4,print.auc=TRUE, print.auc.y=50,
                   print.auc.x=30, quiet = TRUE)

plot.roc(y.test, glm.ELNET.predict, add=TRUE, legacy.axes=TRUE, 
                   percent=TRUE, xlab="False Positive %", ylab="True Positive %",
                   col="rosybrown2", lwd=4, print.auc=TRUE, print.auc.y=40,
                   print.auc.x=30, quiet = TRUE)


legend("bottomright",legend=c("glm.FULL","glm.BACKWARD","glm.ELNET"),
       col=c("firebrick4","indianred3","rosybrown2"),lwd=4)



```
Although the results are very close, the AUC values indicate that the logistic classifier model with all the features outperformed backward and shrinkage models in means of better generalization. 

To better analyse the difference between the model performances we can calculate and compare the evaluation metrics.

```{r logreg_eval_func}
# Function to evaluate prediction models at different thresholds
evaluate.model.performance <- function(predictions, y.test, thresholds, title) {
  output.list <- list()

  # Looping through each threshold to generate predictions and store in output.list
  for (threshold in thresholds) {
    output <- ifelse(predictions > threshold, 1, 0)
    output.list[[as.character(threshold)]] <- output
  }

  # Initialize a data frame to store the evaluation metrics for each threshold
  results <- data.frame(
    Threshold = numeric(length(thresholds)),
    Accuracy = numeric(length(thresholds)),
    F1.Score = numeric(length(thresholds)),
    Precision = numeric(length(thresholds)),
    Recall = numeric(length(thresholds))
  )

  # Compute evaluation metrics for each threshold
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    predict.output <- output.list[[as.character(threshold)]]
    
    # Store results in the results dataframe
    results[i, "Threshold"] = threshold
    results[i, "Accuracy"] = mean(predict.output == y.test)
    results[i, "F1.Score"] = (2 * sum((predict.output == 1 & y.test == 1)) /
                                (2 * sum((predict.output == 1 & y.test == 1)) + 
                                 sum(predict.output == 1 & y.test == 0) + 
                                 sum(predict.output == 0 & y.test == 1)))
    results[i, "Precision"] = sum(predict.output == 1 & y.test == 1) / sum(predict.output == 1)
    results[i, "Recall"] = sum(predict.output == 1 & y.test == 1) / sum(y.test == 1)
  }

  # Rounding results
  results[, c("Accuracy", "F1.Score", "Precision", "Recall")] <- round(results[, c("Accuracy", "F1.Score", "Precision", "Recall")], 4)

  # Return the results table
  kable(results, align = "c", caption = c("Performance Metrics",title))
}
```

```{r logreg_evals, echo=FALSE}
thresholds <- c(0.3, 0.4, 0.5, 0.6, 0.7)

# 1. Basic Logistic Classifier
evaluate.model.performance(glm.FULL.predict, y.test, thresholds, "glm.FULL")

# 2. Feature Selection with Backward Stepwise Search
evaluate.model.performance(glm.BACKWARD.predict, y.test, thresholds, "glm.BACKWARD")

# 3. Elastic Net Shrinkage Method
evaluate.model.performance(glm.ELNET.predict, y.test, thresholds, "glm.ELNET")

```

All of the models have the highest F1 score at threshold level 0.4. And F1 score with 0.4 threshold for glm.ELNET is higher than other models on the test set.

## Another Classification Model

# Model Results

## Performance Metrics and Confusion Matrix

# Conclusion
