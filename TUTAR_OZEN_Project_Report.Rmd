---
title: "Statistical Learning Final Project"
subtitle: "Employee Attrition Classification"

author: [Zeynep TUTAR - 2106038, Aysenur Oya ÖZEN - 0000000]

date: "09-07-2024"
toc: true
toc-own-page: true
number_sections: true
df_print: kable
fontsize: 10pt
lang: en-GB
titlepage: true
titlepage-background: "img/unipdbg.pdf"
header-center: "\\leftmark"
header-right: "\\thetitle"
footer-left: "\\hspace{1cm}"
footer-center: "Page \\thepage"
footer-right: "\\hspace{1cm}"
hyperrefoptions:
  - linktoc=all
biblio-style: apsr
colorlinks: true

editor_options: 
  markdown: 
    wrap: 55
    

header-includes:
   - \usepackage{setspace}
   - \newpage
   - \usepackage{hyperref}
   - \usepackage{multirow}
   - \usepackage{amsmath}
   - \usepackage{algorithm}
   - \usepackage{algorithmic}
   - \usepackage{authblk}
   - \usepackage{graphicx}
   - \usepackage{fancyvrb}
   - \RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{breaklines=true,breakanywhere=true}



output:
  pdf_document:
    template: "template/eisvogel.tex"
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      tidy.opts = list(width.cutoff = 72),
                      out.width = 'textwidth',
                      tidy = TRUE, comment = '',
                      fig.path='figs/',
                      cache = TRUE, cache.path = '_cache/')

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "small","\n\n", x, "\n\n \\normalsize")
})
```

# Introduction to Dataset

The aim of this project is to develop two predictive
models to determine employee attrition of a company.
The dataset[^1] used for this project is a simulated
dataset designed for the analysis and prediction of
employee attrition. It contains detailed information
about various aspects of an employee's profile,
including demographics, job-related features, and
personal circumstances.The dataset contains 74,498
samples. Each record includes a unique Employee ID and
features that influence employee attrition. The goal is
to understand the factors contributing to attrition and
develop predictive models to identify at-risk
employees.

[^1]: <https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset/data>

The dataset is already split into train and test but in
order to better understand the data, it is crucial to
analyse the dataset as a whole.

```{r import_data}

# import the train and test datasets
data_train <- read.csv("data/train.csv", stringsAsFactors=TRUE)
data_test <- read.csv("data/test.csv", stringsAsFactors = TRUE)

# merge the datasets
data <- rbind(data_train, data_test)
attach(data)
```

## Description of the Features

The features of the dataset are presented below:

-   **Employee ID:** A unique identifier assigned to
    each employee.

-   **Age:** The age of the employee, ranging from 18
    to 60 years.

-   **Gender:** The gender of the employee

-   **Years at Company:** The number of years the
    employee has been working at the company.

-   **Monthly Income:** The monthly salary of the
    employee, in dollars.

-   **Job Role:** The department or role the employee
    works in, encoded into categories such as Finance,
    Healthcare, Technology, Education, and Media.

-   **Work-Life Balance:** The employee's perceived
    balance between work and personal life, (Poor,
    Below Average, Good, Excellent)

-   **Job Satisfaction:** The employee's satisfaction
    with their job: (Very Low, Low, Medium, High)

-   **Performance Rating:** The employee's performance
    rating: (Low, Below Average, Average, High)

-   **Number of Promotions:** The total number of
    promotions the employee has received.

-   **Distance from Home:** The distance between the
    employee's home and workplace, in miles.

-   **Education Level:** The highest education level
    attained by the employee: (High School, Associate
    Degree, Bachelor’s Degree, Master’s Degree, PhD)

-   **Marital Status:** The marital status of the
    employee: (Divorced, Married, Single)

-   **Job Level:** The job level of the employee:
    (Entry, Mid, Senior)

-   **Company Size:** The size of the company the
    employee works for: (Small,Medium,Large)

-   **Company Tenure:** The total number of years the
    employee has been working in the industry.

-   **Remote Work:** Whether the employee works
    remotely: (Yes or No)

-   **Leadership Opportunities:** Whether the employee
    has leadership opportunities: (Yes or No)

-   **Innovation Opportunities:** Whether the employee
    has opportunities for innovation: (Yes or No)

-   **Company Reputation:** The employee's perception
    of the company's reputation: (Very Poor, Poor,Good,
    Excellent)

-   **Employee Recognition:** The level of recognition
    the employee receives:(Very Low, Low, Medium, High)

-   **Attrition:** Whether the employee has left the
    company, encoded as 0 (stayed) and 1 (Left).

# Data Analysis

In order to develop predictive models, first it is
necessary to perform exploratory data analysis (EDA)
and modify the format of the data if necessary.

```{r EDA}
# first column contains Employee IDs, so not necessary for summary
# Descriptive statistics of DataFrame
summary(data[,-1])

# Columns in DataFrame
colnames(data[,-1])

# Data types of columns
str(data[,-1])

# Number of unique values in each column
unique_values <- apply(data, 2, function(x) length(unique(x)))
print(unique_values)

```

## Data Preprocessing

To prepare the dataset for further analysis, several
data preprocessing steps are performed:

1.  Converting categorical features to factors
2.  Removing features
3.  Handling na values
4.  etc...

```{r}
#EDA

```

### Outliers

```{r}
#EDA

```

### Visualization

```{r}
#EDA

```

As a result of the analysis, the following observations
were made regarding the characteristics of the data:

## Features vs. Target

### Categorical Features vs. Target

### Numerical Features vs. Target

## Correlation Matrix

### Partial Correlation Matrices

# Data Preparation

After completing the data analysis steps, it is
necessary to prepare the data for model development.

## Handling Categorical Features

In order to use the categorical features in the model,
we need to convert categorical features to numeric (ordinal or nominal) representations.

```{r numeric}

# Ordinal mappings:
balance.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Work.Life.Balance <- balance.map[as.numeric(data$Work.Life.Balance)]

satisfaction.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Job.Satisfaction <- satisfaction.map[as.numeric(data$Job.Satisfaction)]

performance.map <- c('Low'= 1, 'Below Average'= 2, 'Average'= 3, 'High'= 4)
data$Performance.Rating <- performance.map[as.numeric(data$Performance.Rating)]

education.map <- c('High School'= 1, 'Associate Degree'= 2, 'Bachelor’s Degree'= 3, 'Master’s Degree'= 4, 'PhD'= 5)
data$Education.Level <- education.map[as.numeric(data$Education.Level)]

level.map <- c('Entry'= 1, 'Mid'= 2, 'Senior'= 3)
data$Job.Level <- level.map[as.numeric(data$Job.Level)]

reputation.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Company.Reputation <- reputation.map[as.numeric(data$Company.Reputation)]

recognition.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Employee.Recognition <- recognition.map[as.numeric(data$Employee.Recognition)]

size.map <- c('Small'= 1, 'Medium'= 2, 'Large'= 3)
data$Company.Size <- size.map[as.numeric(data$Company.Size)]

# Nominal mappings:
# Create dummy variables for nominal data
data_numeric <- model.matrix( ~ ., data = data) 

# Convert the resulting matrix back to a data frame
data_numeric <- as.data.frame(data_numeric)[,-1]  # -1 to remove the intercept column

```


## Train-Test-Split

Before splitting the data into training and test, first
features and target should be defined.

```{r x_y_split}
# Splitting data into features and target:
X <- data_numeric[, !(colnames(data_numeric) %in% c("Employee.ID", "AttritionStayed"))]

y <- data_numeric$AttritionStayed
```

Now, we can split the dataset for modelling.

```{r train_test_split}
set.seed(42)

trainIndex <- sample(1:nrow(X), 0.8*nrow(X))

# 80% of data is used for training
X.train <- X[trainIndex,]
y.train <- y[trainIndex]

# 20% of data is used for testing
X.test <- X[-trainIndex,]
y.test <- y[-trainIndex]
```

Before moving to modelling step, it is beneficial to
check the dimensions and balance of the datasets.

```{r sanity_check}
# Number of samples in train data
dim(X.train)
train.size <- dim(X.train)[1]

# Number of samples in test data
dim(X.test)
test.size <- dim(X.test)[1]

# Proportion of stayed employees for train data
prop.table(table(y.train))

# Proportion of stayed employees for test data
prop.table(table(y.test))
```

We can observe that the train and test datasets are
balanced within themselves. Also the train data is
representative of test data.

# Predictive Classification Models

Predictive classification models are a type of machine
learning algorithm used to predict the category or
class label of new, unseen instances based on
historical data. These models are trained using a
labelled dataset where the input features (independent
variables) are associated with known class labels
(dependent variable). The goal of the model is to learn
the relationship between the features and the class
labels so that it can accurately classify new data
points into one of the predefined categories.

In this project we aim to find the risk of an employee
leaving the company (class 0) and the factors affecting
employee retention. So we will develop several
classification models and examine their performances.

## Logistic Regression

The logistic regression model estimates the odds of the
dependent variable occurring and applies the logit
(log-odds) transformation to express this relationship.

$$
g(\pi_i) = \text{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right)     \in (-\infty, +\infty)
$$

### Basic Logistic Classifier

```{r}
logit.out <- glm(y.train ~ ., data = X.train, family = binomial)

summary(logit.out)
```
The above results indicate that some features are insignificant to Attrition, such as; Education.LevelBachelor's Degree, Company.SizeMedium, 


### Logistic Regression with Backward Variable Selection

### Logistic Regression with Shrinkage Method

### ROC Curve & Comparison of Logistic Classifiers

## Another Classification Model

# Model Results

## Performance Metrics and Confusion Matrix

# Conclusion
