---
title: "Statistical Learning Final Project"
subtitle: "Employee Attrition Classification"

author: [Zeynep TUTAR - 2106038, Aysenur Oya ÖZEN - 0000000]

date: "09-07-2024"
toc: true
toc-own-page: true
number_sections: true
df_print: kable
fontsize: 10pt
lang: en-GB
titlepage: true
titlepage-background: "img/unipdbg.pdf"
header-center: "\\leftmark"
header-right: "\\thetitle"
footer-left: "\\hspace{1cm}"
footer-center: "Page \\thepage"
footer-right: "\\hspace{1cm}"
hyperrefoptions:
  - linktoc=all
biblio-style: apsr
colorlinks: true

editor_options: 
  markdown: 
    wrap: 72
    

header-includes:
   - \usepackage{setspace}
   - \newpage
   - \usepackage{hyperref}
   - \usepackage{multirow}
   - \usepackage{amsmath}
   - \usepackage{algorithm}
   - \usepackage{algorithmic}
   - \usepackage{authblk}
   - \usepackage{graphicx}
   - \usepackage{fancyvrb}
   - \RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{breaklines=true,breakanywhere=true}



output:
  pdf_document:
    template: "template/eisvogel.tex"
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, message = FALSE,
                      tidy.opts = list(width.cutoff = 72),
                      tidy = TRUE, comment = '',
                      fig.keep = 'all',
                      fig.path='figs/',
                      cache = TRUE, cache.path = '_cache/')

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "small","\n\n", x, "\n\n \\normalsize")
})
```

# Introduction to Dataset

The aim of this project is to develop two predictive
models to determine employee attrition of a company.
The dataset[^1] used for this project is a simulated
dataset designed for the analysis and prediction of
employee attrition. It contains detailed information
about various aspects of an employee's profile,
including demographics, job-related features, and
personal circumstances.The dataset contains 74,498
samples. Each record includes a unique Employee ID and
features that influence employee attrition. The goal is
to understand the factors contributing to attrition and
develop predictive models to identify at-risk
employees.

[^1]: <https://www.kaggle.com/datasets/stealthtechnologies/employee-attrition-dataset/data>

The dataset is already split into train and test but in
order to better understand the data, it is crucial to
analyse the dataset as a whole.

```{r import_data}

# import the train and test datasets
data_train <- read.csv("data/train.csv", stringsAsFactors=TRUE)
data_test <- read.csv("data/test.csv", stringsAsFactors = TRUE)

# merge the datasets
data <- rbind(data_train, data_test)
attach(data)
```

## Description of the Features

The features of the dataset are presented below:

-   **Employee ID:** A unique identifier assigned to
    each employee.

-   **Age:** The age of the employee, ranging from 18
    to 60 years.

-   **Gender:** The gender of the employee

-   **Years at Company:** The number of years the
    employee has been working at the company.

-   **Monthly Income:** The monthly salary of the
    employee, in dollars.

-   **Job Role:** The department or role the employee
    works in, encoded into categories such as Finance,
    Healthcare, Technology, Education, and Media.

-   **Work-Life Balance:** The employee's perceived
    balance between work and personal life, (Poor,
    Below Average, Good, Excellent)

-   **Job Satisfaction:** The employee's satisfaction
    with their job: (Very Low, Low, Medium, High)

-   **Performance Rating:** The employee's performance
    rating: (Low, Below Average, Average, High)

-   **Number of Promotions:** The total number of
    promotions the employee has received.

-   **Distance from Home:** The distance between the
    employee's home and workplace, in miles.

-   **Education Level:** The highest education level
    attained by the employee: (High School, Associate
    Degree, Bachelor’s Degree, Master’s Degree, PhD)

-   **Marital Status:** The marital status of the
    employee: (Divorced, Married, Single)

-   **Job Level:** The job level of the employee:
    (Entry, Mid, Senior)

-   **Company Size:** The size of the company the
    employee works for: (Small,Medium,Large)

-   **Company Tenure:** The total number of years the
    employee has been working in the industry.

-   **Remote Work:** Whether the employee works
    remotely: (Yes or No)

-   **Leadership Opportunities:** Whether the employee
    has leadership opportunities: (Yes or No)

-   **Innovation Opportunities:** Whether the employee
    has opportunities for innovation: (Yes or No)

-   **Company Reputation:** The employee's perception
    of the company's reputation: (Very Poor, Poor,Good,
    Excellent)

-   **Employee Recognition:** The level of recognition
    the employee receives:(Very Low, Low, Medium, High)

-   **Attrition:** Whether the employee has left the
    company, encoded as 0 (stayed) and 1 (Left).

# Data Analysis

In order to develop predictive models, first it is
necessary to perform exploratory data analysis (EDA)
and modify the format of the data if necessary.

```{r libraries}
# installing required libraries
library(car)
library(dplyr)
library(corrplot)
```


```{r summary}

# Descriptive statistics of DataFrame
summary(data)

# Data types of columns
str(data)

```

## Data Preprocessing

To prepare the dataset for further analysis, several
data preprocessing steps are performed:

  1.  Removing features
```{r ID_drop}
# first column contains Employee IDs, so not necessary for analysis
data <- data[, !names(data) %in% "Employee.ID"]
```

  2. Numeric and categorical value separation
```{r num_cat}
numeric_vars <- sapply(data, is.numeric)
categoric_vars <- sapply(data, function(x) is.factor(x) || is.character(x))

# Taking names of features
categoric_var_names <- names(data)[categoric_vars]
numeric_var_names <- names(data)[numeric_vars]

# Numeric val. summary
summary(data[, numeric_vars])
```

  3. Handling missing values
```{r NA_val}
# Missing Values --- No null Values
na_summary <- sapply(data, function(x) sum(is.na(x)))
na_summary

```

### Categorical Features

```{r cat_f}
# Categorical val. dist.
categoric_var_names <- names(data)[categoric_vars]
for (var in categoric_var_names) {
  cat("\nDistribution of", var, ":\n")
  print(table(data[[var]]))
}

# Categorical val. dist.--barplot
par(mfrow = c(2, 2))  
for (cat_var in categoric_var_names) {
  barplot(table(data[[cat_var]]), main=paste(cat_var, "Distribution"), xlab=cat_var, col="firebrick4")
}

```
```{r, include=FALSE}
par(mfrow = c(1, 1)) 
```


### Numeric Features
```{r num_f}
# Numeric features--hist graph
plots_per_page <- 4
num_plots <- length(numeric_var_names)
num_pages <- ceiling(num_plots / plots_per_page)

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 2))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    hist(data[[num_var]], main=paste(num_var, "Distribution"), xlab=num_var, col="firebrick4", breaks=30)
    
    plot_index <- plot_index + 1
  }
}
```
```{r, include=FALSE}
par(mfrow = c(1, 1))  
```


### Target Values
```{r target}
# Target values
par(mfrow = c(1, 2))  

barplot(table(data$Attrition), main="Attrition Count", xlab="Attrition", ylab="Count", col="firebrick4")

# Target dist - Pie chart
attrition_table <- table(data$Attrition)
attrition_df <- as.data.frame(attrition_table)
colnames(attrition_df) <- c("Attrition", "Count")
attrition_df$Percentage <- round(100 * attrition_df$Count / sum(attrition_df$Count), 1)

pie(attrition_df$Count, labels=paste(attrition_df$Attrition, " - ", attrition_df$Percentage, "%"), col=c("firebrick4", rgb(red = 155/255, green = 0/255, blue = 20/255, alpha = 0.3)), main="Attrition Distribution", cex=1, radius=1)

 
```
```{r, include=FALSE}
par(mfrow = c(1, 1)) 
```

```{r, include=FALSE}
plots_per_page <- 4
num_plots <- length(numeric_var_names)
num_pages <- ceiling(num_plots / plots_per_page)
```

```{r target-num}
# Target Visualization with Numeric features Outlier Check --boxplot
cat_var <- "Attrition"

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 2))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    boxplot(data[[num_var]] ~ data[[cat_var]], main=paste(num_var, "by", cat_var), xlab=cat_var, ylab=num_var, col=rgb(red = 155/255, green = 0/255, blue = 20/255, alpha = 0.3))
    plot_index <- plot_index + 1
  }
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

```{r, include=FALSE}
plots_per_page <- 2
num_plots <- length(numeric_var_names)
num_pages <- ceiling(num_plots / plots_per_page)
```


```{r}
# Density plots
cat_var <- "Attrition"

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 1))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    plot(density(data[[num_var]][data[[cat_var]] == "Left"], na.rm = TRUE), col="red", main=paste(num_var, "Density by", cat_var), xlab=num_var, ylab="Density")
    lines(density(data[[num_var]][data[[cat_var]] == "Stayed"], na.rm = TRUE), col="blue")
    legend("topright", legend=c("Left", "Stayed"), col=c("red", "blue"), lty=1, cex=0.8)
    plot_index <- plot_index + 1
  }
}
par(mfrow = c(1, 1))
```


### Outliers

```{r outliers}
# Outlier Analysis 
par(mfrow = c(2, 2))  
for (num_var in numeric_var_names) {
  boxplot(data[[num_var]], main=paste(num_var, "Boxplot"), xlab=num_var, col=rgb(red = 155/255, green = 0/255, blue = 20/255, alpha = 0.3))
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

```{r}
# Function to identify outliers using IQR
identify_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  outliers <- x[x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)]
  return(outliers)
}

# Identify and show outliers for each numeric variable
outliers_list <- list()
for (var in numeric_var_names) {
  outliers <- identify_outliers(data[[var]])
  outliers_list[[var]] <- outliers
  cat("\nOutliers in", var, ":\n")
  print(outliers)
}
```

```{r, include=FALSE}
plots_per_page <- 4
num_plots <- length(numeric_var_names)
num_pages <- ceiling(num_plots / plots_per_page)
```

```{r}
# Plot histograms and highlight outliers

plot_index <- 1
for (page in 1:num_pages) {
  par(mfrow = c(2, 2))  
  for (i in 1:plots_per_page) {
    if (plot_index > num_plots) break
    num_var <- numeric_var_names[plot_index]
    
    hist(data[[num_var]], main=paste(num_var, "Distribution"), xlab=num_var, col="cyan", breaks=30)
    outliers <- outliers_list[[num_var]]
    if (length(outliers) > 0) {
      points(outliers, rep(0, length(outliers)), col="red", pch=16)
    }
    
    plot_index <- plot_index + 1
  }
}
```

```{r, include=FALSE}
par(mfrow = c(1, 1))
```

As a result of the analysis, the following observations
were made regarding the characteristics of the data:

-
-
-
-
-
-
-

## Features vs. Target

### Categorical Features vs. Target

### Numerical Features vs. Target

## Correlation Matrix

```{r Cor.Cov.}
# Cor. and Cov.
cov_matrix <- cov(data[, numeric_var_names])
cor_matrix <- cor(data[, numeric_var_names])

print("Covariance Matrix:")
print(cov_matrix)
print("Correlation Matrix:")
print(cor_matrix)

corrplot(cor_matrix, method = 'number', type = "upper", tl.col = "black", tl.srt = 45)
```


### Partial Correlation Matrices

# Data Preparation

After completing the data analysis steps, it is
necessary to prepare the data for model development.

## Handling Categorical Features

In order to use the categorical features in the model,
we need to convert categorical features to numeric
(ordinal or nominal) representations.

```{r mapping}

# Ordinal mappings:
balance.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Work.Life.Balance <- balance.map[as.numeric(data$Work.Life.Balance)]

satisfaction.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Job.Satisfaction <- satisfaction.map[as.numeric(data$Job.Satisfaction)]

performance.map <- c('Low'= 1, 'Below Average'= 2, 'Average'= 3, 'High'= 4)
data$Performance.Rating <- performance.map[as.numeric(data$Performance.Rating)]

education.map <- c('High School'= 1, 'Associate Degree'= 2, 'Bachelor’s Degree'= 3, 'Master’s Degree'= 4, 'PhD'= 5)
data$Education.Level <- education.map[as.numeric(data$Education.Level)]

level.map <- c('Entry'= 1, 'Mid'= 2, 'Senior'= 3)
data$Job.Level <- level.map[as.numeric(data$Job.Level)]

reputation.map <- c('Poor'= 1, 'Fair'= 2, 'Good'= 3, 'Excellent'= 4)
data$Company.Reputation <- reputation.map[as.numeric(data$Company.Reputation)]

recognition.map <- c('Low'= 1, 'Medium'= 2, 'High'= 3, 'Very High'= 4)
data$Employee.Recognition <- recognition.map[as.numeric(data$Employee.Recognition)]

size.map <- c('Small'= 1, 'Medium'= 2, 'Large'= 3)
data$Company.Size <- size.map[as.numeric(data$Company.Size)]

# Nominal mappings:
# Create dummy variables for nominal data
data_numeric <- model.matrix( ~ ., data = data) 

# Convert the resulting matrix back to a data frame
data_numeric <- as.data.frame(data_numeric)[,-1]  # -1 to remove the intercept column

```

## Train-Test-Split

Before splitting the data into training and test, first
features and target should be defined.

```{r x_y_split}
# Splitting data into features and target:
X <- data_numeric[, !(colnames(data_numeric) %in% c("Employee.ID", "AttritionStayed"))]

y <- data_numeric$AttritionStayed
```

Now, we can split the dataset for modelling.

```{r train_test_split}
set.seed(42)

trainIndex <- sample(1:nrow(X), 0.8*nrow(X))

# 80% of data is used for training
X.train <- X[trainIndex,]
y.train <- y[trainIndex]

# 20% of data is used for testing
X.test <- X[-trainIndex,]
y.test <- y[-trainIndex]
```

Before moving to modelling step, it is beneficial to
check the dimensions and balance of the datasets.

```{r sanity_check}
# Number of samples in train data
dim(X.train)
train.size <- dim(X.train)[1]

# Number of samples in test data
dim(X.test)
test.size <- dim(X.test)[1]

# Proportion of stayed employees for train data
prop.table(table(y.train))

# Proportion of stayed employees for test data
prop.table(table(y.test))
```

We can observe that the train and test datasets are
balanced within themselves. Also the train data is
representative of test data.

# Predictive Classification Models

Predictive classification models are a type of machine learning algorithm used to predict the category or class label of new, unseen instances based on historical data. These models are trained using a labelled dataset where the input features (independent variables) are associated with known class labels (dependent variable). The goal of the model is to learn the relationship between the features and the class labels so that it can accurately classify new data points into one of the predefined categories.

In this project we aim to find the risk of an employee leaving the company (class 0) and the factors affecting employee retention. So we will develop several classification models and examine their performances.

## Logistic Regression

The logistic regression model estimates the odds of the dependent variable occurring and applies the logit (log-odds) transformation to express this relationship.

$$
g(\pi_i) = \text{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right)     \in (-\infty, +\infty)
$$

### Basic Logistic Classifier

```{r}
# First of all we check the model statistics with all the features
glm.FULL <- glm(y.train ~ ., data = X.train, family = binomial)

summary(glm.FULL)
```

The above model statistics indicate that p-values of Company.Tenure and Employee Recognition are above 0.5 indicating that these features are insignificant to the
results. Additionally, some Job.Roles and
Monthly.Income also have high p-values indicating that their effect on Attrition is less significant compared to other features. However, for now we would like to keep all the features in the model and apply feature selection later.

In order to understand how well the model fits the data we can make use of \( R^2 \) statistics. \( R^2 \) provides an indication of how well the independent variables in the
model explain the variability of the dependent
variable. A higher \( R^2 \) value indicates a better fit of the model to the data. The formula for \( R^2 \) is:

$$ R^2 = 1 - \frac{RSS}{ESS} $$

Where:

-   ${RSS}$ is the sum of squares of the residuals (the differences between observed and predicted values), i.e. the deviance of the fitted model
-   ${ESS}$ is the total sum of squares due to regression (the differences between the observed values and the mean of the observed values) 

```{r full_r2}
R2 <- 1 - (summary(glm.FULL)$deviance/summary(glm.FULL)$null.deviance)
R2
```

With the full model the value of \( R^2 \) 0.2352228 indicates that approximately 23.52% of the variance in the target can be explained by the features in the model. Since 23.52% is relatively low, it suggests that the model is not capturing much of the underlying pattern in the data.

Multicollinearity can be a reason for a low \( R^2 \) value, as it can make it difficult to determine the individual effect of each predictor on the target. Calculating the Variance Inflation Factor (VIF) can help to check for multicollinearity among the features.

```{r vif}
library(car)
#vif(glm.FULL)

vif.FULL <- data.frame(features = names(vif(glm.FULL)), VIF = vif(glm.FULL),row.names = NULL)
vif.FULL[order(-vif.FULL$VIF), ]
```
A VIF value of 1 indicates no correlation, values between 1 and 5 indicate moderate correlation and values above 5 suggest significant multicollinearity, which can lead to unreliable coefficient estimates. Most variables have VIF values close to 1, indicating very low or no multicollinearity. A few variables have VIF values between 1 and 5, suggesting moderate multicollinearity, which may not pose serious issues but should be monitored. These variables include Job.RoleTechnology, Job.RoleHealthcare, Monthly.Income, Job.RoleFinance, Marital.StatusSingle, Marital.StatusMarried, Job.RoleMedia and Years.at.Company. These are mostly dummy features of nominal variables and dummy variables are often correlated because they represent categories of the same nominal variable.

### Logistic Regression with Backward Variable Selection

Backward variable selection is a greedy search algorithm used to develop a predictive model by iteratively removing the least significant features The goal is to find the best subset of features that contribute to the model while eliminating those that do not improve its performance. 

Since already one dummy variable for each category is dropped, removing additional dummy variables can lead to loss of information. Therefore, we should drop the features that represent the same category while performing backward selection.


```{r backward_1}
# Backward Step 1: Remove Job.Roles
# Job.Role features both have relatively high VIF values and high p-values

glm.back.1 <- update(glm.FULL, .~.-Job.RoleTechnology-Job.RoleHealthcare-Job.RoleMedia-Job.RoleFinance)

vif.back.1 <- data.frame(features = names(vif(glm.back.1)), VIF = vif(glm.back.1),row.names = NULL)
vif.back.1[order(-vif.back.1$VIF), ]
```

```{r backward_2}
# Backward Step 2: Remove Marital.Status

glm.back.2 <- update(glm.back.1, .~.-Marital.StatusSingle-Marital.StatusMarried)

vif.back.2 <- data.frame(features = names(vif(glm.back.2)), VIF = vif(glm.back.2),row.names = NULL)
vif.back.2[order(-vif.back.2$VIF), ]

```

At this point all the VIF values above 2 have been removed. We should also check for the model statistics.

```{r}
summary(glm.back.2)
```

Company.Tenure and Employee.Recognition are insignificant to the model. So we can drop them too. 


```{r backward_3}
# Backward Step 3: Remove Company.Tenure

glm.back.3 <- update(glm.back.2, .~.-Company.Tenure-Employee.Recognition)

summary(glm.back.3)

```
Although we decreased the VIF scores and p-values, unfortunately, both RSS and AIC scores increased and \( R^2 \) dropped to 0.1592564. So, the models ability to capture the underlying pattern within the dataset decreased. Therefore the greedy search did not converge to an optimal model.


### Logistic Regression with Shrinkage Method

### ROC Curve & Comparison of Logistic Classifiers

## Another Classification Model

# Model Results

## Performance Metrics and Confusion Matrix

# Conclusion
